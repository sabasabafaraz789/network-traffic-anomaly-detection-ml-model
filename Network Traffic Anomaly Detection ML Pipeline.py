# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1veJ_1oxKZcDMvYugW3Yp-fDCVDxeMGvJ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import joblib

# Load and prepare data
file_path = '/content/embedded_system_network_security_dataset.csv'
df = pd.read_csv(file_path)
display(df.head())

# Use the FULL dataset
dd_full = df.drop(columns=['mean_packet_size'])

# Convert boolean columns to int
bool_cols = dd_full.select_dtypes(include='bool').columns
for col in bool_cols:
    dd_full[col] = dd_full[col].astype(int)

print("Dataset shape:", dd_full.shape)
print("Label distribution:")
print(dd_full['label'].value_counts())


print(" PORT FEATURE ANALYSIS ===")

# 1. Analyze port distributions by label
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
for label in [0, 1]:
    subset = dd_full[dd_full['label'] == label]
    plt.hist(subset['src_port'], bins=50, alpha=0.7, label=f'Label {label}')
plt.title('Source Port Distribution by Label')
plt.xlabel('Source Port')
plt.ylabel('Frequency')
plt.legend()

plt.subplot(1, 2, 2)
for label in [0, 1]:
    subset = dd_full[dd_full['label'] == label]
    plt.hist(subset['dst_port'], bins=50, alpha=0.7, label=f'Label {label}')
plt.title('Destination Port Distribution by Label')
plt.xlabel('Destination Port')
plt.ylabel('Frequency')
plt.legend()

plt.tight_layout()
plt.show()

# 2. Create port-based engineered features
print("Creating port-based engineered features...")

# Common service ports (expand based on your domain knowledge)
SUSPICIOUS_PORTS = [22, 23, 21, 25, 53, 80, 443, 8080, 8443, 38405, 65031, 36872, 50700, 41998, 50704, 54291, 37398, 5143, 61465, 11808, 20005, 29736, 23607, 27707, 9798, 13897, 24140, 32848, 38483, 8279, 1113, 26209, 46696, 40552, 42093, 26735, 42096, 49786, 53883, 17531, 29833, 33932, 9357, 40077, 48270, 1168, 15508, 51868, 47260, 10401, 28837, 3751, 59560, 23720, 50350, 61624, 40145, 38102, 49377, 29411, 50921, 42732, 15085, 22256, 53509, 31495, 62728, 13067, 42258, 44821, 32028, 11049, 8498, 30522, 19777, 60227, 20824, 60253, 32096, 38754, 31596, 7022, 26486, 42870, 1912, 45947, 3965, 41860, 16773, 13196, 4494, 6033, 57246, 52638, 35745, 21925, 33701, 4014, 35758, 32688, 20915, 35252, 30660, 22999, 3057, 52725, 3064, 51199]

def create_port_features(df):
    """Create engineered features based on port analysis"""

    # Suspicious port combinations
    df['suspicious_src_port'] = df['src_port'].isin(SUSPICIOUS_PORTS).astype(int)

    return df

# Apply port feature engineering
dd_enhanced = create_port_features(dd_full.copy())
print(f"Enhanced dataset shape: {dd_enhanced.shape}")
display(dd_enhanced.head())

import os

save_directory = '/content'

file_path = os.path.join(save_directory, 'dd_enhanced.csv')

dd_enhanced.to_csv(file_path, index=False)

print(f"'dd_enhanced' DataFrame saved to {file_path}")

dd_label_0 = dd_enhanced[dd_enhanced['label'] == 1]
display(dd_label_0)

print(dd_label_0['suspicious_src_port'].unique())
print(dd_label_0['suspicious_src_port'].value_counts())



# MODEL TRAINING WITH ENHANCED FEATURES


# Separate features and target
X = dd_enhanced.drop('label', axis=1)
y = dd_enhanced['label']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ENHANCED MODEL TRAINING WITH PORT-FOCUSED FEATURE IMPORTANCE


models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss'),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

# Enhanced parameter grids with focus on feature importance
param_grids = {
    'Random Forest': {
        # 'n_estimators': [100, 200, 300],
        # 'max_depth': [10, 20, None],
        # 'min_samples_split': [2, 5],
        # 'class_weight': ['balanced', None]
        'n_estimators': [100],
        'random_state': [42],
        'max_depth':[10],
        'min_samples_split': [5]
    },
    'XGBoost': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.1, 0.2],
        'scale_pos_weight': [1, (y_train == 0).sum() / (y_train == 1).sum()]
    }
}

results = {}
best_models = {}

for name, model in models.items():
    print(f"\n=== Training {name} ===")

    if name in param_grids:
        grid_search = GridSearchCV(
            model, param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1
        )
        grid_search.fit(X_train_scaled, y_train)
        best_model = grid_search.best_estimator_
        print(f"Best parameters: {grid_search.best_params_}")
    else:
        best_model = model
        best_model.fit(X_train_scaled, y_train)

    # Predictions
    y_pred = best_model.predict(X_test_scaled)
    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]

    # Store results
    results[name] = {
        'model': best_model,
        'accuracy': (y_pred == y_test).mean(),
        'roc_auc': roc_auc_score(y_test, y_pred_proba),
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }

    best_models[name] = best_model
    print(f"\n{name} Results:")
    print(f"{name} Accuracy: {results[name]['accuracy']}")
    print(f"{name} ROC-AUC: {results[name]['roc_auc']}")


# DETAILED PORT FEATURE ANALYSIS


# Identify best model
best_model_name = max(results, key=lambda name: results[name]['roc_auc'])
best_model = best_models[best_model_name]

print(f"\n*** Best Model: {best_model_name} ***")
print(f"Best Model Accuracy: {results[best_model_name]['accuracy']}")
print(f"Best Model ROC-AUC: {results[best_model_name]['roc_auc']}")

# Detailed feature importance analysis
if hasattr(best_model, 'feature_importances_'):
    # Use X.columns for feature names as the model was trained on all scaled features
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nTop 15 Most Important Features:")
    print(feature_importance.head(15))

    # Plot feature importance
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(15)
    colors = ['red' if 'port' in feat.lower() else 'blue' for feat in top_features['feature']]

    bars = plt.barh(range(len(top_features)), top_features['importance'])
    for i, bar in enumerate(bars):
        bar.set_color(colors[i])

    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance')
    plt.title(f'Top 15 Feature Importance - {best_model_name}\n(Red = Port-related features)')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

    # Analyze port feature importance specifically
    port_importance = feature_importance[feature_importance['feature'].str.contains('port', case=False)]
    print(f"\nPort Feature Importance Ranking:")
    print(port_importance)


# PORT-SPECIFIC BUSINESS INSIGHTS


print("\n=== PORT-RELATED SECURITY INSIGHTS ===")

# Analyze which specific ports are most indicative of attacks
port_attack_analysis = dd_enhanced.groupby('label').agg({
    'src_port': ['mean', 'std', lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else None],
    'dst_port': ['mean', 'std', lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else None]
}).round(2)

print("Port Statistics by Label:")
print(port_attack_analysis)

# Most common ports for each label
print("\nMost common source ports by label:")
for label in [0, 1]:
    common_ports = dd_enhanced[dd_enhanced['label'] == label]['src_port'].value_counts().head(5)
    print(f"Label {label}: {list(common_ports.index)}")

print("\nMost common destination ports by label:")
for label in [0, 1]:
    common_ports = dd_enhanced[dd_enhanced['label'] == label]['dst_port'].value_counts().head(5)
    print(f"Label {label}: {list(common_ports.index)}")


# SAVE MODELS


joblib.dump(best_model, 'port_enhanced_anomaly_detection_model.pkl')
joblib.dump(scaler, 'port_enhanced_scaler.pkl')
# joblib.dump('port_enhanced_feature_selector.pkl')

print("\nEnhanced models saved successfully!")

# Final evaluation
y_pred_best = results[best_model_name]['y_pred']
y_pred_proba_best = results[best_model_name]['y_pred_proba']

print("\n=== Final Best Model Evaluation ===")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_best))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_best))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()